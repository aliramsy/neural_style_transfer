{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = models.vgg19(weights= models.VGG19_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace=True)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg_model.features[22:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.model = model.features[:29]\n",
    "        self.chosen_features = [0, 5, 10, 19, 28]\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_features = []\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "\n",
    "            if i in self.chosen_features:\n",
    "                output_features.append(x)\n",
    "\n",
    "        return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((300, 300)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = transform(Image.open('./images/picaso_style.jpeg')).unsqueeze(0)\n",
    "content_image = transform(Image.open('./images/catO.jpg')).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = content_image.clone().requires_grad_(True)\n",
    "#generated_image = torch.randn(content_image.shape, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(vgg_model).eval()\n",
    "steps = 1001\n",
    "alpha = 1\n",
    "beta = .5\n",
    "optimizer = optim.Adam([generated_image], lr= .005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of Usage:\n",
    "\n",
    "*     torch.no_grad(): Affects all operations within the block—computation graphs are not created for any * * tensors or operations within the context of with torch.no_grad().\n",
    "    .detach(): Affects only the specific tensor on which it is called, detaching it from the computation graph. Subsequent operations on the detached tensor will not track gradients, but prior operations may still have their graphs.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "    torch.no_grad(): Use this when you don't need any gradients at all (e.g., during inference or when computing values that don't require backpropagation, like in your style/content feature extraction).\n",
    "    .detach(): Use this when you want to selectively exclude specific tensors from having their gradients tracked, while other parts of the model still need gradients. It’s typically useful when you want to freeze parts of a model during training or when you’re reusing outputs in a new context without backpropagating through the same part of the model.\n",
    "\n",
    "Memory Efficiency:\n",
    "\n",
    "    Both methods prevent the creation of computation graphs, thus saving memory. However, torch.no_grad() is more efficient if you're doing inference for a batch of operations where no gradients are needed, as it disables all gradient tracking within its scope.\n",
    "    .detach() is useful when you need gradients for some parts of the graph but want to stop tracking for certain tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2645e+08, grad_fn=<AddBackward0>)\n",
      "tensor(77107296., grad_fn=<AddBackward0>)\n",
      "tensor(35573192., grad_fn=<AddBackward0>)\n",
      "tensor(19159724., grad_fn=<AddBackward0>)\n",
      "tensor(12029507., grad_fn=<AddBackward0>)\n",
      "tensor(8390893., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#content_features = model(content_image).detach()\n",
    "#style_features = model(style_image).detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    content_features = model(content_image)\n",
    "    style_features = model(style_image)\n",
    "\n",
    "for step in range(steps):\n",
    "    #content_features = model(content_image)\n",
    "    #style_features = model(style_image)\n",
    "    generated_features = model(generated_image)\n",
    "\n",
    "    style_loss = content_loss = total_loss = 0\n",
    "\n",
    "    for generated_feature, content_feature, style_feature in zip(\n",
    "        generated_features, content_features, style_features\n",
    "    ):\n",
    "\n",
    "        batch_size, channel, height, width = generated_feature.shape\n",
    "        content_loss += torch.mean(torch.square(generated_feature -\n",
    "                                                content_feature))\n",
    "\n",
    "        # content_loss += torch.square(generated_feature -\n",
    "        #                                        content_feature)/2\n",
    "\n",
    "        generated_gram = generated_feature.view(channel, height * width).mm(\n",
    "            generated_feature.view(channel, height * width).t()\n",
    "        )\n",
    "\n",
    "        style_gram = style_feature.view(channel, height * width).mm(\n",
    "            style_feature.view(channel, height * width).t()\n",
    "        )\n",
    "        style_loss += torch.mean(torch.square(generated_gram - style_gram))\n",
    "        # style_loss += (torch.square(generated_gram - style_gram))/(2 * channel * width * height)\n",
    "\n",
    "    total_loss = alpha * content_loss + beta * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(total_loss)\n",
    "        generated_cat = denormalize(generated_image)\n",
    "        save_image(generated_cat, \"./pic_cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
